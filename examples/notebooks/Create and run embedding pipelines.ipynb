{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2613554",
   "metadata": {},
   "source": [
    "# Creating and running embedding pipelines\n",
    "\n",
    "`bluegraph` allows to create emebedding pipelines (using the `EmbeddingPipeline` class) that represent a useful wrapper around a sequence of steps necessary to produce embeddings and compute point similarities. In the example below we create a pipeline for producing `attri2vec` node embeddings and computing their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b75b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bluegraph.core import PandasPGFrame\n",
    "\n",
    "from bluegraph.preprocess.encoders import ScikitLearnPGEncoder\n",
    "from bluegraph.backends.stellargraph import StellarGraphNodeEmbedder\n",
    "from bluegraph.downstream.similarity import (SimilarityProcessor,\n",
    "                                             FaissSimilarityIndex,\n",
    "                                             ScikitLearnSimilarityIndex,\n",
    "                                             SimilarityIndex)\n",
    "from bluegraph.downstream import EmbeddingPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1543c",
   "metadata": {},
   "source": [
    "## Example 1: creating pipeline trainable with `run_fitting`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754e35f",
   "metadata": {},
   "source": [
    "We first create an encoder object that will be used in our pipeline to encode node property `definition` using a TfIdf encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5452e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_encoder = ScikitLearnPGEncoder(\n",
    "    node_properties=[\"definition\"],\n",
    "    text_encoding_max_dimension=512,\n",
    "    text_encoding=\"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43780a91",
   "metadata": {},
   "source": [
    "We then create an embedder object that can compute node embeddings for input graphs using `attri2vec` node embedding technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ff22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 128\n",
    "params = {\n",
    "    \"length\": 5,\n",
    "    \"number_of_walks\": 10,\n",
    "    \"epochs\": 5,\n",
    "    \"embedding_dimension\": D\n",
    "}\n",
    "attri2vec_embedder = StellarGraphNodeEmbedder(\n",
    "    \"attri2vec\", feature_vector_prop=\"features\", edge_weight=\"npmi\", **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2fb8d",
   "metadata": {},
   "source": [
    "Next, we create a similarity processor based of Faiss indices that allows us to perform fast search for nearest neighbors according to our embedding vectors. We set our similarity measure to _cosine similarity_.\n",
    "\n",
    "__Note:__ in the code below we use the `SimilarityProcessor` interface and not `NodeSimilarityProcessor`, as we have done it in previous tutorials. We use this lower abstraction level interface, because the `EmbeddingPipeline` is designed to work with any embedding models (not only node embedding models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5418d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_processor = SimilarityProcessor(\n",
    "    FaissSimilarityIndex(\n",
    "        similarity=\"cosine\", dimension=D, n_segments=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcdea4",
   "metadata": {},
   "source": [
    "And finally we create a pipeline object that stacks all the above-mentioned elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbd5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "attri2vec_pipeline = EmbeddingPipeline(\n",
    "    preprocessor=definition_encoder,\n",
    "    embedder=attri2vec_embedder,\n",
    "    similarity_processor=similarity_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6eeef3",
   "metadata": {},
   "source": [
    "Now, let us load the training graph from the provided example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d12d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = PandasPGFrame.load_json(\"../data/cooccurrence_graph.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3640f",
   "metadata": {},
   "source": [
    "We run the fitting process, which given the input data performs the following steps:\n",
    "1. fits the encoder\n",
    "2. transforms the data\n",
    "3. fits the embedder\n",
    "4. produces the embedding table\n",
    "5. fits the similarity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51767e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7f970e2d6f90> >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oshurko/opt/anaconda3/envs/bg/lib/python3.7/site-packages/bluegraph/downstream/similarity.py:180: SimilarityWarning: Faiss segmented index is not trained, training on the provided vectors\n",
      "  SimilarityIndex.SimilarityWarning)\n"
     ]
    }
   ],
   "source": [
    "attri2vec_pipeline.run_fitting(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416dc1f",
   "metadata": {},
   "source": [
    "We can save our pipeline to the file system as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fde8649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/oshurko/opt/anaconda3/envs/bg/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../data/attri2vec_test_model/embedder/model/assets\n"
     ]
    }
   ],
   "source": [
    "attri2vec_pipeline.save(\n",
    "    \"../data/attri2vec_test_model\",\n",
    "    compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29c8b7",
   "metadata": {},
   "source": [
    "We can launch prediction of the unseen graph nodes using our pipeline as follows (in this case we use the same graph). As an output, we obtain embedding vectors produced by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e70af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = attri2vec_pipeline.run_prediction(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f26a2d",
   "metadata": {},
   "source": [
    "## Example 2: creating manually trained pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3c1d95",
   "metadata": {},
   "source": [
    "In the previous example we used `FaissSimilarityIndex` and the backend for our nearest neighbors search. `Faiss` indices are updatable and allow us to add new points to the index at any point. Therefore, we were able to create an 'untrained' pipeline stacking preprocessor, embedder and empty similarity index. We then run all the training steps at once by using `run_fitting`. As the result, vectors output by the embedder were added to the index, once they were produced.\n",
    "\n",
    "However, in some cases, similarity indices are static and the set of vectors on which they are built must be provided at the creation time. Consider the following example.\n",
    "\n",
    "We would like to use [BallTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html) index implemented in `scikit-learn` and provided by `bluegraph`'s `ScikitLearnSimilarityIndex`. In the cell below we try to initialize this index without initial vectors on which it must be built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dbf6ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught the following error: \n",
      "Initial vectors must be provied (scikit learn indices are not updatable) \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sklearn_similarity_processor = SimilarityProcessor(\n",
    "        ScikitLearnSimilarityIndex(\n",
    "            similarity=\"poincare\", dimension=D,\n",
    "            index_type=\"ballktree\", leaf_size=10)\n",
    "    )\n",
    "except SimilarityIndex.SimilarityException as e:\n",
    "    print(\"Caught the following error: \")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ddc92",
   "metadata": {},
   "source": [
    "This means that we cannot create an initially empty similarity index and let our pipeline fill it with vectors once the embedder has output the them. What we can do instead is run encoding and embedding manually, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee7f27c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n"
     ]
    }
   ],
   "source": [
    "transformed_graph = definition_encoder.fit_transform(graph)\n",
    "embedding = attri2vec_embedder.fit_model(transformed_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f93b3",
   "metadata": {},
   "source": [
    "We now can create a similarity index on the produced embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15569073",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_similarity_processor = SimilarityProcessor(\n",
    "    ScikitLearnSimilarityIndex(\n",
    "        similarity=\"poincare\", dimension=D,\n",
    "        initial_vectors=embedding[\"embedding\"].tolist(),\n",
    "        index_type=\"ballktree\", leaf_size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f6fcf",
   "metadata": {},
   "source": [
    "And, finally, stack our steps into a pipeline that can be dumped and re-used as in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a44b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attri2vec_sklearn_pipeline = EmbeddingPipeline(\n",
    "    preprocessor=definition_encoder,\n",
    "    embedder=attri2vec_embedder,\n",
    "    similarity_processor=sklearn_similarity_processor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bg",
   "language": "python",
   "name": "bg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
