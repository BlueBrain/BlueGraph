{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfIdf-based lexical embedding pipeline\n",
    "\n",
    "In this example notebook we will illustrate how Tf-Idf encoding based on character n-grams of aliases from the [NCIt](https://ncithesaurus.nci.nih.gov/ncitbrowser/) ontology can be used to constuct embeddings of words and lexical similarity search using BlueGraph's `EmbeddingPipeline`.\n",
    "\n",
    "(Credits to [Pierre-Alexandre Fonta](https://github.com/pafonta) for the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from joblib import parallel_backend\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "from bluegraph.downstream.data_structures import EmbeddingPipeline\n",
    "from bluegraph.downstream.similarity import SimilarityProcessor\n",
    "from bluegraph.downstream.data_structures import Preprocessor\n",
    "from bluegraph.preprocess.utils import TfIdfEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open NCIT ontology terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../data/NCIT_ontology.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9617e4453280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/NCIT_ontology.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0montology\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '../data/NCIT_ontology.json'"
     ]
    }
   ],
   "source": [
    "path = \"../data/NCIT_ontology.json\"\n",
    "if not os.path.isfile(path):\n",
    "    shutil.unpack_archive(path + \".zip\", extract_dir=path)\n",
    "\n",
    "with open(\"../data/NCIT_ontology.json\", \"r\") as f:\n",
    "    ontology = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all unique aliases (all lower case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliases = list(set(alias.lower() for k, v in ontology.items() for alias in v[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a sample of ~100000 aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "terms_to_include = [\n",
    "    \"covid-19 infection\",\n",
    "    \"covid-19\",\n",
    "    \"glucose\",\n",
    "    \"sars-cov-2\"\n",
    "]\n",
    "random_sample = random.sample(aliases, N)\n",
    "aliases_to_train = list(set(random_sample + terms_to_include))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify Tf-Idf model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"analyzer\": \"char\",\n",
    "    \"dtype\": np.float32,\n",
    "    \"max_df\": 1.0,\n",
    "    \"min_df\": 0.0001,\n",
    "    \"ngram_range\": (3, 3),\n",
    "    \"max_features\": 2048\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `EmbeddingPipeline` using:\n",
    "\n",
    "- `TfIdfEncoder` as a preprocessor,\n",
    "- No embedder\n",
    "- BlueGraph `SimilarityProcessor` with Euclidean distance based on an index segmented into 100 Voronoi cells (more details can be found [here](https://github.com/facebookresearch/faiss/wiki/Faster-search))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = EmbeddingPipeline(\n",
    "    preprocessor=TfIdfEncoder(params),\n",
    "    embedder=None,\n",
    "    similarity_processor=SimilarityProcessor(\n",
    "        similarity=\"euclidean\", dimension=2048, n_segments=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run fitting of the pipeline on the selected subset of aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oshurko/opt/anaconda3/envs/bg/lib/python3.7/site-packages/bluegraph/downstream/similarity.py:169: SimilarityWarning: Similarity index is not trained, training on the provided vectors\n",
      "  SimilarityProcessor.SimilarityWarning)\n"
     ]
    }
   ],
   "source": [
    "pipeline.run_fitting(aliases_to_train, index=aliases_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve embedding vectors for the trems of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"glucose\", \"covid-19 infection\", \"lalala not in index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pipeline.retrieve_embeddings(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector sizes: \n",
      "\t'glucose': 2048\n",
      "\t'covid-19 infection': 2048\n",
      "\t'lalala not in index': None\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector sizes: \")\n",
    "for i, v in enumerate(vectors):\n",
    "    print(\"\\t'{}': {}\".format(terms[i], len(v) if v is not None else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, distances = pipeline.get_similar_points(\n",
    "    existing_indices=[\"glucose\", \"covid-19 infection\", \"lalala not in index\"],\n",
    "    k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar points: \n",
      "\t'glucose': ['u-13c-glucose', 'glucose', '2h glucose', 'deoxyglucose', '2 hr glucose']\n",
      "\t'covid-19 infection': ['covid-19 infection', 'hpv infection', 'hpv16 infection', 'hpv-16 infection', 'gum infection']\n",
      "\t'lalala not in index': None\n"
     ]
    }
   ],
   "source": [
    "print(\"Similar points: \")\n",
    "for i, p in enumerate(points):\n",
    "    print(\"\\t'{}': {}\".format(terms[i], list(p) if p is not None else None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save(\"../data/Cord-19-NCIT-linking\", compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict vectors for potentially unseen points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pipeline.run_prediction(\n",
    "    [\"hello\", \"darkness\", \"my old\", \"friend\", \"glucose\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get similar points for these vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Index(['shellfish', 'shell', 'helg', 'hel113', 'hel-s-153w', 'hel-s-43',\n",
       "         'hel-s-28', 'helsnf1', 'hel-s-61p', 'hel-9'],\n",
       "        dtype='object'),\n",
       "  Index(['dar', 'edar', 'witness', 'sourness', 'shakiness', 'nes1', 'darc',\n",
       "         'helplessness', 'avodart', 'dart'],\n",
       "        dtype='object'),\n",
       "  Index(['dj400n23', 'tcf3b', 'i(17q)', 'phq0311', 'nr1c1', 'mrd7', 'nf1',\n",
       "         'au-011', 'hba1a', 'dxs239'],\n",
       "        dtype='object'),\n",
       "  Index(['girlfriend', 'wd5-making new friends', 'wd7-making new friends',\n",
       "         'brief-p', 'voxorien', 'votrient', 'nutrifriend cachexia',\n",
       "         'friend of gata 2', 'nr1c1', 'tcf3b'],\n",
       "        dtype='object'),\n",
       "  Index(['u-13c-glucose', 'glucose', '2h glucose', 'deoxyglucose',\n",
       "         '2 hr glucose', 'glucobay', 'glucosuria', 'glucclr', 'eagluc', 'gluc'],\n",
       "        dtype='object')],\n",
       " [array([0.4691298 , 0.4691298 , 0.76471364, 0.76471364, 0.76471364,\n",
       "         0.76471364, 0.76471364, 0.76471364, 0.76471364, 0.76471364],\n",
       "        dtype=float32),\n",
       "  array([0.67307055, 0.67307055, 0.77463806, 0.80448186, 0.8209591 ,\n",
       "         0.84367234, 0.8581717 , 0.9287    , 0.9622607 , 0.9622607 ],\n",
       "        dtype=float32),\n",
       "  array([1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001,\n",
       "         1.0000001, 1.0000001, 1.0000001, 1.0000001], dtype=float32),\n",
       "  array([0.        , 0.5868373 , 0.5868373 , 0.72098583, 0.77514833,\n",
       "         0.7829668 , 0.93820626, 0.96719193, 1.        , 1.        ],\n",
       "        dtype=float32),\n",
       "  array([0.        , 0.        , 0.12738925, 0.1278751 , 0.25717676,\n",
       "         0.6263409 , 0.6397846 , 0.67013925, 0.67013925, 0.67013925],\n",
       "        dtype=float32)])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_similar_points(vectors=vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict vectors for potentially unseen points and add them to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oshurko/opt/anaconda3/envs/bg/lib/python3.7/site-packages/bluegraph/downstream/similarity.py:188: SimilarityWarning: Points Index(['glucose'], dtype='object') already exist in the index, ignoring...\n",
      "  SimilarityProcessor.SimilarityWarning)\n"
     ]
    }
   ],
   "source": [
    "vectors = pipeline.run_prediction(\n",
    "    [\"hello\", \"darkness\", \"my old\", \"friend\", \"glucose\"],\n",
    "    add_to_index=True,\n",
    "    data_indices=[\"hello\", \"darkness\", \"my old\", \"friend\", \"glucose\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bg",
   "language": "python",
   "name": "bg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
